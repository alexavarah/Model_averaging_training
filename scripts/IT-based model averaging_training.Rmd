---
title: "IT-based model averaging"
author: "Alexa Varah"
date: "05/01/2021"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Information theoretic (IT)-based model averaging allows inferences to be drawn from weighted support over several models (multi-model inference, MMI).

# Concept

The data is 'correct'.  
We’re looking to see which effects (parameters) are supported by the data.  
We’re aiming for the data-based selection of a set of best models. In other words, we'll examine the likelihood of various models (i.e., various hypotheses about reality), given the data. Inference is based on more than one model.   

**Why information theoretic?** We use existing knowledge to inform _a priori_ the choice of predictor variables in our model (i.e., _before we look at the data_!), rather than using the data to inform our choice of variables. 

**Why model averaging?** The benefit of using multiple models is that they can be ranked and scaled to show model uncertainty over the set. *All* models are then used for inference. "If one has a large number of closely related models, such as in linear-regression based variable selection .... designation of a single best model is unsatisfactory because that 'best' model is often highly variable. That is, the model estimated to be best would vary from data set to data set, where replicate data sets would be collected under the same underlying process. In this situation, model averaging provides a relatively much more stabilized inference." (p.151, Burnham & Anderson 2002).

IT-based MMI is good for:  

* observational data 
*	data with lots of potential predictor variables  
*	messy data  
*	small datasets  

... in other words, many ecological data sets.

Although IT-based MMI is not completely free of the disadvantages of stepwise methods, it suffers less from the following drawbacks of stepwise methods:  

* Stepwise methods ignore model uncertainty (they test only a fraction of all possible models). This problem is exacerbated in smaller datasets and in inter-related predictors.  
*	Stepwise methods use subjective critical values (e.g. p < 0.05), leading to the assumption of zero effects of terms not in the final model. This is unlikely to be true for ecological data.  
*	Stepwise methods can lead to parameter estimation bias, with the risk that the final model will contain overestimated effect sizes.  
&nbsp;


The process involves _a priori_ model specification, then model selection, and finally estimation of parameters and their precision.  
&nbsp; 

We'll assume that we've identified possible predictor variables through a literature search. *N.B.* This should be done _before_ you even do fieldwork! Once you've collected data you can do some basic data exploration (but NO data dredging or modelling!).  

We'll now load the dataset and get modelling.


# Load packages and data

Load some packages.
```{r Load-packages, message=FALSE}
#R.Version() 
rm(list=ls())
library(readxl) # for reading in Excel workbooks / spreadsheets
library(dplyr) # for easy data manipulation (includes packages dplyr, tidyr, stringr, ggplot2, purr, tibble, readr, forcats)
#library(chron) # for handling dates
library(stringr)
library(lubridate) # for working with dates
library(MASS) # for stats
library(lme4) # for mixed effects models
library(arm) # for standardizing variables
library(MuMIn) # for multi-model inference
library(knitr)
library(kableExtra) # for working with tables
```


We're going to work with a dataset collected by pan trapping in English agroforestry and monoculture fields. It's on Figshare [here](https://figshare.com/articles/dataset/Transect_walk_data_collected_in_English_agroforestry_and_monoculture_fields_/28770587). Meta-data is given on Figshare, but in brief the dataset contains counts of butterflies, wild bees and hoverflies from five farms in England. Each farm had an agroforestry and monoculture (control) treatment. Sampling was mainly done over two years (one site was sampled once in the third year).  
  
For now, just load the data. Maybe save it in Figshare once it's all combined.

Load and tidy the solitary bee data.
```{r}
bee_data <- read.csv('../data/solitary_bee_data.csv', header = TRUE) %>% 
  dplyr::mutate(
    date = as.Date(date, format = "%d/%m/%Y"),
    site = factor(site),
    treatment = factor(treatment),
    year = factor(year)
  )

str(bee_data)
```


Look at how many sites and years are in this data set.
```{r}
table(bee_data$site, bee_data$year)
```


```{r}
table(bee_data$site, bee_data$month, bee_data$year)
```

Transform the data to give total counts of each species by site, treatment and date  
```{r Collapse-bee-data-over-treatment, message=FALSE}
solbee_counts_by_tmt <- bee_data %>% 
  dplyr::group_by(paste(site, treatment, date, sep = "_")) %>%
  dplyr::count(sp) %>% 
  # transform data from long to wide
  tidyr::pivot_wider(names_from = sp, values_from = n) %>% 
  # replace NAs with zero
  replace(is.na(.), 0) %>% 
  # rename the ID column we created
  dplyr::rename(ID = "paste(site, treatment, date, sep = \"_\")")

head(solbee_counts_by_tmt, n=10)
```

Load the site data.
```{r}
site_data <- readRDS('../data/site_data.rds')
str(site_data)
```

Collapse this so there is one row for each visit per site and treatment. Retain just the variables we need.
```{r}
site_data <- site_data %>% 
  distinct(site, treatment, year, month, visit, .keep_all = TRUE) %>% 
  dplyr::select(site, treatment, year, month, date, visit, landuse, field, af_age:snh) %>% 
  dplyr::mutate(month = as.factor(month))

str(site_data)
```

Join the solitary bee data onto the site metadata. We have to do it this way around as the solitary bee data only records bees observed and so there will be missing sampling events where no bees were observed. However, we want to retain all sampling dates, giving zero to any dates where no bees were observed.
```{r}
# Separate the key column into site, treatment, date
solbee <- solbee_counts_by_tmt %>%
  tidyr::separate(ID, into = c("site", "treatment", "date"), sep = "_", extra = "merge") %>%
  dplyr::mutate(
    date = trimws(date),  # Remove any leading/trailing whitespace
    date = as.Date(date, format = "%Y-%m-%d"),
    year = lubridate::year(date),
    year = as.factor(year),
    month = lubridate::month(date),
    month = as.factor(month)
  ) %>%
  dplyr::relocate(year:month, .after = treatment)

str(solbee)
```

Which sites have more than one visit in a month?
```{r}
duplicates_table <- solbee %>%
  dplyr::group_by(site, treatment, year, month) %>%
  dplyr::filter(n() > 1) %>%
  dplyr::ungroup()

# Display the table nicely
kable(duplicates_table %>% 
        dplyr::select(site:date), format = "html") %>%
  kable_styling(font_size = 12, bootstrap_options = c("striped", "hover")) #%>%
  #row_spec(0, bold = TRUE, underline = TRUE)
```

Export the table.
```{r}
write.csv(duplicates_table, "../output/duplicates_table.csv")
```



Create a new 'visit' column
```{r}
# solbee_new <- solbee %>%
#   dplyr::group_by(site, treatment, year) %>%
#   dplyr::mutate(visit = row_number()) %>%
#   dplyr::ungroup() %>% 
#   dplyr::relocate(visit, .after = date)
```


Deal with sampling occasions/ visits.
* at RR MC replace visit == 8 on 2011-08-31 with visit == 9  
* at SD, remove 2012-08-16. Not sure why I have two observations of solitary bees from this date.
* at WAF, remove data from 22/08/2012 -> I can't remember why I did this extra visit in August. It's OK to use the data from 10/08/2012 (this was the day Patrick did it whilst I was at SD, even though the control was a field of flowering clover and the AF wasn't in flower), as most solitary bees don't forage on clover.
* at WH, the visit on 21/08/2011 is the 'September' visit so change month to 9 in both treatments.    

```{r}
# RR	MC	2011	8	2011-08-31 <- change month to 9
# SD  AF  2012  8 2012-08-17 <- I didn't go to SD on 17th August 2012. Remove this row.
# WAF	AF	2012	6	2012-06-28 <- change month to 7
# WAF	AF	2012	8	2012-08-10 <- remove this date as I re-did it (the control field was full of flowering clover, the AF field wasn't)
# NB also need to change WAF	AF	2012	8	2012-08-10 to WAF	AF	2012	8	2012-08-23 in the site data
# WAF	MC	2012	6	2012-06-01 <- change month to 5
# WH	AF	2011	8	2011-08-22 <- change month to 9
# WH	MC	2011	8	2011-08-22 <- change month to 9

solbee_newmonths <- solbee %>%
  # 1. Fix typo in date
  # mutate(date = case_when(
  #   site == "SD"  & treatment == "AF" & year == 2012 & date == as.Date("2012-08-17") ~ as.Date("2012-08-10"),
  #   TRUE ~ date
  # )) %>%
  
  # 2. Recalculate months
  mutate(month = month(date)) %>%
  
  # 3. Remove incorrect / un-usable rows
  # the control field was full of flowering clover, the AF field wasn't
  filter(!(site == "WAF" & treatment == "AF" & year == 2012 & date == as.Date("2012-08-10"))) %>%
  # I didn't go to SD on 17th August 2012
  filter(!(site == "SD" & treatment == "AF" & year == 2012 & date == as.Date("2012-08-17"))) %>% 

  # 4. Apply specific month corrections if needed
  mutate(month = case_when(
    site == "RR"  & treatment == "MC" & year == 2011 & date == as.Date("2011-08-31") ~ 9,
    #site == "WAF" & treatment == "AF" & year == 2012 & date == as.Date("2012-06-28") ~ 7,
    site == "WAF" & treatment == "MC" & year == 2012 & date == as.Date("2012-06-01") ~ 5,
    site == "WAF" & treatment == "AF" & year == 2012 & date == as.Date("2012-06-01") ~ 5,
    site == "WH"  & treatment == "AF" & year == 2011 & date == as.Date("2011-08-22") ~ 9,
    site == "WH"  & treatment == "MC" & year == 2011 & date == as.Date("2011-08-22") ~ 9,
    TRUE ~ month
  ))

str(solbee_newmonths)
```


Check whether any sites have more than one visit in a month.
```{r}
duplicates_table_newmonths <- solbee_newmonths %>%
  dplyr::group_by(site, treatment, year, month) %>%
  dplyr::filter(n() > 1) %>%
  dplyr::ungroup()

# Display the table nicely
kable(duplicates_table_newmonths %>% 
        dplyr::select(site:date), format = "html") %>%
  kable_styling(font_size = 12, bootstrap_options = c("striped", "hover"))
```

No duplicated months.


Make these same adjustments to the site data.
```{r}

```



# Now join with the site_data
```{r}
df <- site_data %>%
  left_join(solbee, by = c("site", "treatment", "year", "month")) %>% 
  dplyr::relocate(landuse:snh, .after = date) %>% 
  # fill blanks with zeros
  dplyr::mutate(across(Andrena_chrysosceles:Lasioglossum_semilaeve, ~tidyr::replace_na(.x, 0))) %>% 
  # create a 'visit' column
  dplyr::group_by(site, treatment, year) %>% 
  dplyr::mutate(visit = row_number()) %>% 
  dplyr::ungroup() %>% 
  dplyr::relocate(visit, .after = date)

str(solbee_data)
```

# Exclude unsuitable sampling events
SD 20th June 2012 - control was next to a flowering borage field so exclude this sampling event.


# superseded

We're going to work with a dataset collected on transect walks from English agroforestry and monoculture fields. It's on Figshare [here](https://figshare.com/articles/dataset/Transect_walk_data_collected_in_English_agroforestry_and_monoculture_fields_/28770587). Meta-data is given on Figshare, but in brief the dataset contains counts of butterflies, wild bees and hoverflies from five farms in England. Each farm had an agroforestry and monoculture (control) treatment. Sampling was mainly done over two years (one site was sampled once in the third year).  
  
We'll download the dataset from Figshare and read it into R.
```{r load-rds, echo=TRUE}
# Specify Figshare URL
url <- "https://figshare.com/ndownloader/articles/28770587/versions/1"

# Create temporary file for ZIP
zip_file <- tempfile(fileext = ".zip")

# Download the ZIP
download.file(url, zip_file, mode = "wb")

# Unzip to a temporary directory
unzip_dir <- tempdir()
unzip(zip_file, exdir = unzip_dir)

# List files inside the ZIP
list.files(unzip_dir, recursive = TRUE)

# Specify the .rds file
rds_path <- file.path(unzip_dir, "transect_data_tidy.rds")  

# Read the RDS file
pollinator_data <- readRDS(rds_path)

# Check the structure
str(pollinator_data)
```


We're just going to use the butterfly data so let's extract that. We will work with total butterfly abundance for this tutorial, so we need to sum the number of butterflies.
```{r}
butterfly_data <- pollinator_data %>% 
  # extract the site variables and the butterfly species
  dplyr::select(c(site:butact, Aglais_urticae:Ochlodes_faunus )) %>% 
  # remove unidentified butterflies and the columns giving totals by genus
  dplyr::select(
    -contains("unidentified", ignore.case = TRUE),
    -c(Lycaenidae, Pieridae, Hesperiidae)
  ) %>% 
  # get total abundance of butterflies
  dplyr::mutate(total_butterflies = rowSums(across(Aglais_urticae:Ochlodes_faunus))) %>% 
  # remove data from WAF in 2011 as there was a problem with this site then
  dplyr::filter(!(site == "WAF" & year == 2011))


bee_data <- pollinator_data %>% 
  # extract the site variables and the butterfly species
  dplyr::select(c(site:beeact, terrestris_lucorum:solitary_bee)) %>% 
  # remove rows with NA
  drop_na(terrestris_lucorum:solitary_bee) %>%
  # convert to integer
  mutate(across(terrestris_lucorum:solitary_bee, as.integer)) %>% 
  # get total abundance of bees
  dplyr::mutate(total_bees = rowSums(across(terrestris_lucorum:solitary_bee)))

```

We'll add in the ages of the agroforestry systems at the first year of sampling.
```{r}
# Create a dataframe that maps site to age
site_age <- data.frame(
  site = c("WH", "SD", "WAF", "RR", "CE", "LHF"),
  af_age = c(2, 9, 14, 24, 24, 18)
)

# Join the ages onto the butterfly data
butterfly_data <- butterfly_data %>%
  left_join(site_age, by = "site") %>% 
  dplyr::relocate(af_age, .after = field)

# Tidy up
rm(site_age)
```

# Inspect data

Have a look at the data.
```{r}
glimpse(butterfly_data)
```


Both treatments were sampled equally:
```{r}
table(butterfly_data$treatment, butterfly_data$year)
```

When were sites sampled?
```{r}
table(butterfly_data$site, butterfly_data$year)
```

Let's look at how many fields were in each site.
```{r}
table(butterfly_data$site, butterfly_data$field)
```

You can see that at some sites, both treatments were in the same field (e.g., CE & RR). At other sites, the treatments were in different fields in different years (e.g., SD, WH).  

We're not going to worry too much about the model structure for this tutorial, but when using your own data you would of course need to think about how to account for  hierarchical designs, unequal sampling, lots of zeros in your data, etc. when considering your approach and model specification.  


```{r Load-and-prep-data, echo=FALSE, message=FALSE, warning = FALSE, results='hide'}
workingdir = "C:/Users/alexv2/OneDrive - Natural History Museum/PhD/PhD manuscripts/Biodiversity"
setwd(workingdir) # output will go into pollination folder

transect = read_xlsx("C:/Users/alexv2/OneDrive - Natural History Museum/PhD/data/Transect data_master.xlsx",2)

lep = subset(transect, select=c(site:butact,LEPIDOPTERA)) %>% # extract just the columns relevant to bumblebee analyses
  rename(butt=LEPIDOPTERA)  %>% 
  mutate(
    site = factor(site),
    site = recode(site, "seh" = "waf"),
    site = recode(site, "mc" = "ce"),
    visit = factor(visit),
    year = factor(year),
    date = as.Date(paste(day, month, year, sep="-"), "%d-%m-%Y"), # concatenate day, month, year to create date
    monthtxt = month.abb[month],
    yrmth = paste(year, month, sep="-"), # create a new vector of month-year
    jdate = difftime(date, as.Date(paste(01, 03, year, sep="-"),"%d-%m-%Y"), units = c("days")), # calc days since 1st March. 
    landuse = factor(landuse),
    treatment = factor(treatment),
    field = factor(field),
    bound.area = as.numeric(bound.area),
    n.hedg.bound = as.numeric(n.hedg.bound),
    snh = as.numeric(snh),
    starttime = times(strftime(start,"%H:%M:%S")), # convert to just time (removes date, which was wrong date anyway)
    endtime = times(strftime(end,"%H:%M:%S")), # convert to just time (removes date, which was wrong date anyway)
    windfac = factor(wind),
    wind3 = factor(1+(windfac=="2")+2*(windfac=="3")+2*(windfac=="4")), # get rid of the one value of 4 in windstrength
    butt = as.numeric(butt) 
  )  %>% 
  filter(!( (year == '2011' & site == 'waf' ) )) %>% # Filter out WAF 2011
  filter(!is.na(butt))  %>% # remove rows where there is an NA in butterfly column  
  subset( select = -c(start, end)) %>% # drop these columns
  droplevels() %>% # get rid of any unused levels, i.e. SEH
  
  # Rescale some of the variables that are on very different scales. Here we rescale them to values between 0 and 1
  # (they need to be numeric before we can rescale them)
  mutate_at('jdate',as.numeric) %>% 
  mutate(resc.bound.area = (bound.area - min(bound.area)) / ( max(bound.area) - min(bound.area) ),
         resc.alleylength = (alleylength - min(alleylength)) / ( max(alleylength) - min(alleylength) ),
         resc.sun = (sun - min(sun)) / ( max(sun) - min(sun) ),
         resc.hedg.dist = (hedg.dist - min(hedg.dist)) / ( max(hedg.dist) - min(hedg.dist) ),
         resc.jdate = (jdate - min(jdate)) / ( max(jdate) - min(jdate) )
         ) %>%
  mutate(ID = paste(site, treatment, visit, sep = '-') )  %>% # make site-treatment-visit identifier
  group_by(ID) %>% # grouping factor for sum
  mutate(butt_total = sum(butt)) %>% # sum butterfly counts per site-treatment-visit combo
  distinct(ID, .keep_all = TRUE) %>% # keep only one row per ID
  droplevels() %>% # drop empty factor levels i.e. SEH
  # pull out the columns we want to keep (N.B. dplyr 'select' can clash with MASS so use 'dplyr::select' to ensure we're using 'select' from dplyr)
  dplyr::select("ID", "site", "visit", "treatment", "landuse", "yrmth", "year", "month", "monthtxt", "date", 
                "butt_total", "boundSR.all", "boundSR.ip", "n.hedg.bound", "snh", "sun", "resc.sun", "hedg.dist", "resc.hedg.dist", 
                "bound.area", "resc.bound.area", "jdate", "resc.jdate", "wind3") %>% 
  droplevels()  %>%
  arrange(site, visit, treatment) # sort data

# add in system age
site <- c('wh','sd','waf', 'rr', 'ce', 'lhf')
age <- c(2,9,14,24,24,18)
df <- data.frame (site, age)
lep <- merge(lep, df, by="site")
```
&nbsp; 

# Model

## 1. Build a highly parameterised global model

…but don’t overfit.

Use the predictor variables identified from (a) the literature, (b) your existing knowledge, and (c) any initial data exploration to build a 'global' model. Ideally, the global model has in it all the variables thought to be important, although it may be impossible to fit a global model if sample size is very small. In this example, the global model is a GLMM with a Poisson error structure because the data are counts of butterflies and have a hierarchical structure (fields within sites). Check goodness-of-fit, overdispersion, correlated variables etc. in the usual way. Because the data set used here is small, we can not include every potential predictor variable, so we will include only those which were likely to have the largest effect on butterfly abundance (based on the previous _a priori_ work).  

```{r Build-global-model-norescaling}
gm_notrescaled <- glmer(total_butterflies ~ treatment + landuse + bound_area + n_hedg_bound + sun + jdate + 
                                     treatment:landuse + (1|site), poisson, data=butterfly_data,
                        na.action = "na.fail", 
                        glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000)))
```

We get a warning about the predictor variables being on different scales. We need to rescale them.  

## 2. Rescale variables
You can do this in several ways (the method you choose _may_ give you different parameter estimates):

* Use the [*scale*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) function in [base](https://www.rdocumentation.org/packages/base/versions/3.6.2) R. *Scale* gives values centered around zero by subtracting the mean of the variable from each value of the variable, then dividing by 1 standard deviation.
* Use the [*rescale*](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) function in the *scales* package, which rescales numeric vectors to have a specified maximum and minimum (default is 0-1).
* Use the [*standardize*](https://www.rdocumentation.org/packages/arm/versions/1.11-2/topics/standardize) function in the [*arm*](https://www.rdocumentation.org/packages/arm/versions/1.11-2/topics/standardize) package, which you can apply to a model rather than individual variables. It centers predictor variables and divides by 2 standard deviations. Numeric variables that take on more than two values are each rescaled to have a mean of 0 and a sd of 0.5; Binary variables are rescaled to have a mean of 0 and a difference of 1 between their two categories; Non-numeric variables that take on more than two values are unchanged; Variables that take on only one value are unchanged.  

We'll use *standardize* to rescale all variables:
```{r Rescale-global-model}
gm <- standardize(gm_notrescaled, standardize.y = FALSE) # we don't want to standardize the response variable.
summary(gm)
```


## 3. Check model fit

Check the model fit in the usual way. Model fit is assessed for the global model ONLY. If the global model fits the data adequately, then a more parsimonious model will also fit the data.
```{r Check-model}
par(mfrow=c(1,1))

qqnorm(resid(gm))
qqline(resid(gm))

hist(residuals(gm))
```

The histogram of residuals you've provided suggests several important characteristics about the residuals from your mixed effects model of butterfly abundance:

1. **Right Skewness (Positive Skew)**

* The histogram is **skewed to the right**, meaning that most residuals are clustered around zero or negative values, but there are a few relatively large positive residuals.
* This skewness indicates that the model **tends to underpredict** butterfly abundance in some cases — i.e., actual counts are higher than predicted.

2. **Non-Normality of Residuals**

* Residuals are not symmetrically distributed around zero, which **violates the normality assumption** of residuals common in linear mixed models.
* This may suggest that a transformation of the response variable (e.g., log or square root) or a **different distributional assumption** (e.g., Poisson, Negative Binomial) might be more appropriate for the data.

3. **High Concentration at Zero**

* There's a strong peak at or near zero, which implies that for many observations, the model predictions are fairly accurate.
* However, the spread of residuals becomes wider in the positive direction, again hinting at underprediction in some observations.

4. **Potential Issues with Model Fit**

* The presence of **outliers or long tails** in residuals (especially in ecological count data like butterfly abundance) is not uncommon, but if excessive, it can signal that the model does not fully capture the underlying data structure.



**Recommendations:**

* **Check for overdispersion** if you're using a Poisson model.
* **Consider fitting a Negative Binomial model** if overdispersion is present.
* Evaluate whether a **zero-inflated model** might be appropriate, especially if many zeros are present in the data.
* **Transform the response variable** (e.g., log-transform) if using a linear model and residuals are highly skewed.
* Explore **random slopes** or interactions if certain patterns are missed.

Check overdispersion
```{r}
library(performance)
check_overdispersion(gm)
```

Fit a negative binomial model.
```{r}
library(glmmTMB)
gm_nb <- glmmTMB(total_butterflies ~ treatment + landuse + bound_area + 
                      n_hedg_bound + sun + jdate + treatment:landuse + (1|site), 
                    data = butterfly_data, 
                    family = nbinom2)
summary(gm_nb)
```

```{r Check-model}
par(mfrow=c(1,1))

qqnorm(resid(gm_nb))
qqline(resid(gm_nb))

hist(residuals(gm_nb))
```


Negative binomial didn't fix the issue.  

Try OLRE.  
```{r}
# Create OLRE
butterfly_data$obs_id <- factor(1:nrow(butterfly_data))


gm_notrescaled_olre <- glmer(total_butterflies ~ treatment + landuse + bound_area + n_hedg_bound + sun + jdate + 
                                     treatment:landuse + (1|site) + (1|obs_id), 
                             poisson, 
                             data = butterfly_data,
                             na.action = "na.fail", 
                             glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000)))

gm_olre <- standardize(gm_notrescaled_olre, standardize.y = FALSE) # we don't want to standardize the response variable.

summary(gm_olre)
```


```{r Check-model}
par(mfrow=c(1,1))

qqnorm(resid(gm_olre))
qqline(resid(gm_olre))

hist(residuals(gm_olre))
```

This is still not OK. 

❌ Interpretation of the Q-Q Plot

This **Q-Q plot of residuals from the Poisson model with OLRE** shows clear **departure from normality**, especially:

* A **distinct upward curve** at the upper end (right tail): the observed residuals are **much higher** than expected under normality.
* A **flattening pattern in the middle**: suggests residuals are more tightly packed than they would be under a normal distribution.
* Overall: residuals **deviate substantially** from the diagonal line, especially in the **upper tail** → your model is **still not adequately capturing the distribution** of the response.

🧠 What This Means

Even though adding an OLRE helped with overdispersion, **the residuals are still not normally distributed** — which suggests:

1. The **Poisson assumption** (mean = variance) may still not be appropriate.
2. The response data may exhibit **greater heterogeneity**, or **zero-inflation**, or may follow a **different distribution**.

✅ Recommended Next Steps

**1. Switch to a Negative Binomial GLMM**

* This is often the best fix for overdispersion and non-normal residuals in count data.
* Use the `glmmTMB` package:


Then examine:

* Residuals: `resid(mod_nb, type = "pearson")`
* Q-Q plot and histogram
* Overdispersion

**2. Consider Zero-Inflated Model (if many zeros)**

If your data includes **a lot of zero counts**, try a **zero-inflated negative binomial** model:

**3. Use DHARMa for Diagnostics**

The [`DHARMa`](https://cran.r-project.org/web/packages/DHARMa/index.html) package simulates residuals from GLMMs and gives **better diagnostics**:
This provides uniformity plots, scaled residuals, and additional tests for overdispersion and zero-inflation.

🧭 Summary

* The Q-Q plot confirms that **Poisson+OLRE isn't enough**.
* The best next step is to fit a **Negative Binomial model**, and possibly test a **zero-inflated version**.
* Use **DHARMa** to rigorously check model diagnostics.


Double-check using Dharma as it gives better diagnostics.
```{r}
library(DHARMa)
sim_res <- simulateResiduals(gm_olre)
plot(sim_res)

#check_model
```

My manually-made QQ plot suggests zero inflation.
```{r}
install.packages("glmmTMB")  # if not already installed
library(glmmTMB)
```

```{r}
mod_zinb <- glmmTMB(
  # fit the main model for non-zero counts.
  total_butterflies ~ treatment + landuse + bound_area + n_hedg_bound + sun + jdate + treatment:landuse + (1 | site), # Count model
  # model probability of extra zeros with just an intercept (i.e., constant across observations)
  ziformula = ~1, # Zero-inflation model (intercept only)
  data = butterfly_data,
  family = nbinom2  # Use nbinom1 if variance ≈ mean * linear, nbinom2 if variance ≈ mean²
)
```

Check model.
```{r}
sim_zinb <- DHARMa::simulateResiduals(mod_zinb)


# Q-Q plot
DHARMa::plotQQunif(sim_zinb)

# Residuals vs predicted values
DHARMa::plotResiduals(sim_zinb$fittedPredictedResponse, sim_zinb$scaledResiduals,
              main = "Residuals vs Predicted")

# Histogram of residuals
hist(sim_zinb$scaledResiduals, main = "Histogram of Scaled Residuals")

# Residuals vs observed response
DHARMa::plotResiduals(sim_zinb$observedResponse, sim_zinb$scaledResiduals,
              main = "Residuals vs Observed Response")
```



&nbsp; 


## 3. Generate all possible lower-dimensional sub-models

...in effect, all possible hypotheses.  

We use the *dredge* function from the [*MuMIN*](https://www.rdocumentation.org/packages/MuMIn/versions/1.43.17) package, which is a multi-model inference package. You won't keep all of the sub-models generated, just those with the highest relative worth.  

This process can take a while (more predictors = more time).  

```{r Generate-model-set}
modset <- dredge(gm, rank = "AICc") # AICc is the default so 'rank =' is obsolete here. Rank options = AIC, QAIC, BIC.
head(modset)
```
(The singular fit messages indicate that, for some of the sub-models, one or more variances are (very close to) zero.)  

The output shows the top 6 models of all the possible sub-models, ranked by AIC~*c*~.  

As well as AIC~*c*~, the output gives the log likelihood ('logLik'), the change in AIC~*c*~ ('delta'), and the Akaike weight ('weight'). The Akaike weight ($w$~*$i$*~) indicates the relative likelihood of a model, *given the data and the set of models*. Akaike weights are computed by normalising the likelihood values, and provide a way of scaling and interpreting the changes in AIC~*c*~ values. Akaike weights of all models in the set add up to 1, so if you add or remove a model from the set you need to recompute $w$~*$i$*~ for all the models in the new set. Unless the top model has $w$~*$i$*~ $\ge$ 0.9, you should use model averaging.

In the output above, we can see that the 'best' model in the set is not convincingly the best model: the evidence ratio for model 108 versus model 112 is only about 2.5 (i.e., $w$~108~ / $w$~112~ = 2.538). This relatively weak support for the 'best' model suggests that we would expect to see a lot of variation in the selected best model from sample to sample if we could draw multiple independent samples; in other words, the data-based model selection uncertainty is likely to be high. This indicates that these data are inadequate to reach strong inference so we should definitely use model averaging to make more robust inference (MMI).   
&nbsp; 

### 4. Create a 'top models' set

By comparing these models against each other to find their relative worth, we can refine the model set created in step 3 above, retaining just those models for which there is good empirical support. In this way we create a smaller set of candidate models to use for inference.  

Models in the set are ranked by AIC (or AIC~*c*~ for small sample sizes, or QAIC for overdispersed data, etc.).  
There is substantial empirical support for a model $i$ with difference in AIC from the top model ($\Delta$~$i$~) of 0-2, considerably less (but still some) support for models with $\Delta$~$i$~ = 4-7 and none for models with $\Delta$~$i$~ > 10 [(Burnham & Anderson 2002)](http://sutlib2.sut.ac.th/sut_contents/H79182.pdf). 

You can choose which cut-off to use to create your set of 'best' or 'top' models. Here we want our 'top models' set to contain only those models that have substantial empirical support so we'll choose $\Delta$~$i$~ $\le$ 3.  

```{r Get-top-models}
topmods <- get.models(modset, subset = delta <3)
```
&nbsp; 

### 5. Use the top models set for inference  

There are four stages to this, the first two of which are done for you by the function *model.avg*:  

**1** Make parameter estimates averaged across the top model set.  
The function *model.avg* computes weighted estimates of the predicted values, weighting the predictions by the Akaike weights. The model output will show two sets of parameter estimates, which have been estimated as follows:  

* **Full average** Average a particular parameter over all models in the top models set, assuming a zero value for the parameter in models where it is absent (this serves to 'shrink' the parameter estimate back towards zero, which is useful in ameliorating model selection bias). Also called 'unconditional' average.  
* **Conditional average** Average a particular parameter over only those models in which it appears (the 'natural' average).  

In other words, the 'full' coefficients set terms to 0 if they are not included in the model while averaging, whereas the 'conditional' coefficients ignores the predictors entirely. The 'full' coefficients are thus more conservative.   

The 'full' method is useful if you're more interested in *which* parameters are having an effect. The 'conditional' (or natural average) method is useful if you're more interested in effect sizes (I'm still looking for the ref for this). In this example, the conclusions will be the same from either method. We'll use the conditional coefficients as I'm interested in effect sizes.  

**2** Quantify model selection uncertainty and incorporate this into estimates of precision.  
If several other independent data sets were collected, would the same top model set be selected? Very possibly not. This model selection uncertainty must be included in measures of precision of parameter estimates and is included in the 'adjusted SE' (aka unconditional SE) values given in the output.  

```{r Model-average}
modavg <- model.avg(topmods)
summary(modavg)
```

**3** Work out the relative importance of predictor variables.  
To calculate the relative variable importance ($w$~*$i$$p$*~) you simply sum the Akaike weights ($w$~*$i$*~) of the models in which the parameter appears.

For example, *treatment* (parameter 2) appears in all models in the top model set. You therefore add the Akaike weights of all models (`r round(Weights(modavg)[1],2)` + `r round(Weights(modavg)[2],2)` + `r round(Weights(modavg)[3],2)` + `r round(Weights(modavg)[4],2)`) to give $w$~*$i$$p$*~ = 1 for *treatment*.  
*Boundary area* (parameter 3) only appears in the second model, which has $w$~*$i$$p$*~ = `r round(Weights(modavg)[2],2)`. The relative importance ($w$~*$i$$p$*~) of *boundary area* is therefore just `r round(Weights(modavg)[2],2)` (i.e. not important).  

The *MuMIn* package does this for you:
```{r Get-relative-variable-importance}
importance(modavg)
```

**4** Calculate confidence intervals:
```{r CIs}
confint(modavg,level = 0.95)
```
  
When reporting your findings, talk about 'strength of evidence'. Avoid using the word 'significant' as we haven't done hypothesis testing and haven't used $p$ values. These have no place in this method. **Do** give estimates of effect size and associated precision.

You could present the results like this:

(N.B. The code to make the results table is long and not pretty so I haven't included it here, but if you ever want it, just ask.)

```{r Make-results-table, echo=FALSE, message=FALSE, warning = FALSE, results='hide'}
#length(coef(modavg)) # 8: intercept and 7 variables

# extract top models and their parameters from top.mod.params
# 'modset' is the output from dredge (i.e. the full model set)
# 'topmods' is the model set resulting from your specified cut-off
top.mod.params  <- data.frame(modset[1:(length(topmods)) ,])
top.mod.params

# extract parameter estimates and change parameter estimates to a bullet point to show which parameters are in which model
# 'modavg' is the output from model averaging
modvars <- top.mod.params[ ,1:(1+length(importance(modavg))) ]
modvars <- data.frame(t(modvars)) # transpose
colnames(modvars) <- as.character(1:length(topmods)) # set column names to the model rank
modvars <- tibble::rownames_to_column(modvars, "Parameter") # convert rownames to a column and name it Parameter
modvars <- modvars %>% mutate_if(is.numeric, funs(ifelse(!is.na(.), "\u2022", ""))) # replace estimated values with bullet symbol

# add model averaged estimates and std error to the results table
# first get model averaged estimates and std errors
rm(results)
results <- data.frame(coefTable(modavg))
results <- tibble::rownames_to_column(results, "Parameter") # convert rownames to a column called 'Parameter'
results <- dplyr::select(results, -df) # drop the last column, which is empty
results[,2:3] <- round(data.frame(results[,2:3]),3) # round estimates to 3 dec pl

# join modvars and results
# NB Parameter values are not exactly matched: change the intercept and any interaction terms so that text matches in both dfs
modvars$Parameter <- gsub('X.Intercept.','(Intercept)', modvars$Parameter)
modvars$Parameter <- gsub('\\.c',':c', modvars$Parameter)

# join the two dfs
results <- dplyr::left_join(results, modvars, "Parameter") %>% 
  relocate(c(Estimate,Std..Error), .after = last_col()) # move the avgd parameter estimates and std errors to the end

# create table of CIs to add into results table
ci <- round(data.frame(confint(modavg,level = 0.95)),3)  
ci <- tibble::rownames_to_column(ci, "Parameter")
ci$X2.5.. <- formatC(as.numeric(ci$X2.5..),format='f',digits=3,flag='0') # convert to character and retain trailing zeros
ci$X97.5.. <- formatC(as.numeric(ci$X97.5..),format='f',digits=3,flag='0') # convert to character and retain trailing zeros

# rename variables and create concatenated 95% CI column
ci <- ci %>% rename(
  LCI = X2.5..,
  UCI = X97.5..
  ) %>% mutate(
  `95%CI` = paste(LCI, UCI, sep = ", "),.keep = "unused")

ci$`95%CI` <- with(ci, paste0("(", `95%CI`, ")")) # add brackets round the CIs
results <- dplyr::left_join(results, ci, "Parameter") # add the 95%CIs into the results table

# add relative variable importances (wip) as another column
varimp <- data.frame(importance(modavg)) # OR: sw(modavg) gives per variable sum of model weights - i.e. rel var importance
varimp <- tibble::rownames_to_column(varimp, "Parameter") # convert rownames to a column called 'Parameter'
varimp$`importance.modavg.` <- round(varimp$`importance.modavg.`,2) # round wip to 2 dec pl
colnames(varimp)[2] <- "wip"
results <- dplyr::left_join(results, varimp, "Parameter") # add the wip into the results table

# add df, logLik, AICc, delta AICc and weight to results table
# first extract goodness of fit data from the top model parameters
modgof <- top.mod.params[ ,which(colnames(top.mod.params)=="df"):which(colnames(top.mod.params)=="weight") ]
modgof$logLik <- formatC(as.numeric(modgof$logLik),format='f',digits=2,flag='0') # convert to character and retain trailing zeros
modgof$AICc <- formatC(as.numeric(modgof$AICc),format='f',digits=2,flag='0') # convert to character and retain trailing zeros
modgof$delta <- formatC(as.numeric(modgof$delta),format='f',digits=2,flag='0') # convert to character and retain trailing zeros
modgof$weight <- formatC(as.numeric(modgof$weight),format='f',digits=2,flag='0') # convert to character and retain trailing zeros
modgof <- data.frame(t(modgof)) # transpose
colnames(modgof) <- as.character(1:length(topmods))

# add 4 more (empty) columns on to match dataframe 'res'
modgof <- modgof %>%
  add_column(Estimate = NA,
             Std..Error = NA,
             `95%CI` = NA,
             wip = NA)

modgof <- tibble::rownames_to_column(modgof, "Parameter") # convert rownames to a column and name it Parameter
results <- rbind(results, modgof) # join the goodnedd of fit coefs onto the bottom of the results table

# some cells in the results table need to be blank.
# first convert relevant columns to character and retain trailing zeros first:
results$Estimate <- formatC(as.numeric(results$Estimate),format='f',digits=3,flag='0') 
results$Std..Error <- formatC(as.numeric(results$Std..Error),format='f',digits=3,flag='0') 
results$wip <- formatC(as.numeric(results$wip),format='f',digits=2,flag='0') 
# now index the cells which need to be blank, and set them to blank:
results[which(results=="df"):which(results=="weight"),which(colnames(results)=="Estimate"):which(colnames(results)=="wip")] <- ""

# improve column names and remove c. and z. from variable names:
results <- results %>% rename(
  "\u03B2" = Estimate,
  SE = Std..Error
  ) %>% 
  mutate(
    Parameter = gsub("c\\.", "", Parameter)) %>% 
  mutate(
    Parameter = gsub("z\\.", "", Parameter)) 

results

#clean up
rm(top.mod.params)
rm(modvars)
rm(ci)
rm(varimp)
rm(modgof)

```


```{r Make-pretty-results-table, echo=FALSE, message=FALSE, warning = FALSE, }
library(kableExtra)

kable(results, booktabs = T, align = c('l', rep('c',4),rep('r',4)) ) %>%
  kable_classic(full_width = F, position = "left", html_font = "Arial") %>%
  add_header_above(c(" ", "Model rank" = 4, "Model averaged coefficients" = 4)) %>% 
  
  column_spec(2:5, width = "1.7cm") %>% 
  column_spec(6:7, width = "1.5cm") %>% 
  column_spec(8, width = "3cm") %>% 
  column_spec(9, width = "1.3cm") %>% 
  #  #row_spec(8, hline_after=T)# this only works if output is LaTex
  row_spec(8, extra_css = "border-bottom: 1px dotted") %>% # this works if output is html
  row_spec(0, extra_css = "border-bottom: 1px solid") 
```
Key: $\beta$, averaged parameter estimate; $w$<sub>$ip$</sub>, relative variable importance; SE, adjusted standard error; df, degrees of freedom; delta, change in AICc from top model; weight, Akaike weight.

&nbsp; 

Useful refs:  
[Burnham & Anderson 2002](https://www.springer.com/gp/book/9780387953649)  
[Symonds & Moussalli 2011](https://link.springer.com/article/10.1007/s00265-010-1037-6)  
[Harrison et al 2018](https://peerj.com/articles/4794/)  
[Burnham et al 2011](https://link.springer.com/article/10.1007/s00265-010-1029-6)  
[Dormann et al 2018](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1309)    


*End.*

```{r build-global-model-scale, eval=FALSE, echo=FALSE, message=FALSE, warning = FALSE}
#Comparing different ways of standardising predictor variables:
##### (a) Using *scale* for just those predictors that are on v different scales to the others
gm.scl <- glmer(butt_total ~ treatment + landuse + scale(bound.area) + n.hedg.bound + sun + scale(jdate) +
                           treatment:landuse +
                           (1|site/visit), poisson, data=lep,
                         na.action = "na.fail", 
                         glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000)))
summary(gm.scl)
# The model output looks pretty good: 

#* there are no warnings about singular fit    

#* no correlation issues in the fixed effects  

#* the standard errors are OK (i.e. small, so model is prob not overfitted)  

#* NB residuals don't look completely normally distributed 
#&nbsp; 
```

```{r build-global-model-stdz, eval=FALSE, echo=FALSE, message=FALSE, warning = FALSE}
##### (b) Using standardize to rescale all numeric predictors in the model

gm.noscale <- glmer(butt_total ~ treatment + landuse + bound.area + n.hedg.bound + sun + jdate + 
                                     treatment:landuse + (1|site/visit), poisson, data=lep,
                        na.action = "na.fail", 
                        glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000)))

gm.stdzafter <- standardize(gm.noscale,standardize.y = FALSE)

gm.stdz <- standardize(glmer(butt_total ~ treatment + landuse + bound.area + n.hedg.bound + sun + jdate + 
                                     treatment:landuse + (1|site/visit), poisson, data=lep,
                        na.action = "na.fail", 
                        glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000))),
                  standardize.y = FALSE)

summary(gm.stdz)
summary(gm.stdzafter)
#When doing it this way, we get warnings about rescaling variables and convergence - these warnings are generated before the variables are rescaled.  
#Because all the numeric variables have been rescaled, we get different parameter estimates.  
```

```{r no-longer-needed, eval=FALSE, echo=FALSE, message=FALSE, warning = FALSE}
#library(help = "MuMIn")
#Weights(modavg) # model weights (Akaike weights)

#library(texreg)
#citation("texreg")
# texreg returns model parameters in order of importance (more or less)
#out <- extract(modavg, use.ci = FALSE, adjusted.se = TRUE)
#out # this gives unconditional (i.e. full) coefs - I want the natural average so I'l have to get them from the coef table


#####
# making results table - superceded
#rm(modelrank)
#modelrank <- as.character(c(1:length(topmods))) # get the rank of all models in the top models set
#results[ , modelrank] <- NA # add a new column for each model. The name of the column will be a number indicating each model's rank.
# re-order columns
#results <- results%>%
#  dplyr::select(-c("Estimate","Std..Error"), everything())


#results$Parameter <- gsub(':','.', results$Parameter)

# use fuzzy joining as names aren't exactly the same
# df x is the good one, y is the one with dodgy spelling
#library(stringr)
#library(fuzzyjoin)
#results %>% fuzzy_inner_join(modvars, by = c("Parameter" = "Parameter"), match_fun = str_detect)
#x %>% fuzzy_inner_join(y, by = c("string" = "seed"), match_fun = str_detect)

# insert columns for each model between cols 1 & 2 of 'results'
#rm(modelrank)
#modelrank <- as.character(c(1:length(topmods))) # get the rank of all models in the top models set
#results[ , modelrank] <- NA # add a new column for each model. The name of the column will be a number indicating each model's rank.
# re-order columns
#results <- results%>%
#  dplyr::select(-c("Estimate","Std..Error"), everything())

```


