---
title: "IT-based model averaging"
author: "Alexa Varah"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    collapsed: no
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
  github_document:
    toc: yes
    toc_depth: 4
    html_preview: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Information theoretic (IT)-based model averaging allows inferences to be drawn from weighted support over several models (multi-model inference, MMI).

# Concept


* The data is 'correct'.  
* We’re looking to see which effects (parameters) are supported by the data.  
* We’re aiming for the data-based selection of a set of best models. In other words, we'll examine the likelihood of various models (i.e., various hypotheses about reality), given the data.  
* Inference is based on more than one model.   
  
  
**Here's the difference between a Frequentist approach and an Information Theoretic approach:**
  
| Aspect              | Frequentist Approach                                            | Information-Theoretic Approach                                    |
|:---------------------|:-------------------------------------------------------------------|:-------------------------------------------------------------------|
| Focus               | Hypothesis testing and inference based on long-run frequencies  | Model selection based on information content and parsimony       |
| Core Idea           | A single "true" model exists; data are random                   |  There is no single "true" model, but some models approximate reality better than others |
| Model Perspective   | Evaluate how likely the data are given a specific model         | Compare models based on how well they explain the data, using information loss as a criterion (e.g., AIC)|
| Probability View    | Probability = long-run frequency*                                | Probability = a measure of information or model fit (via AIC)     |
  
*Long-run frequency: If you repeated the experiment infinitely many times, in X% of those, you'd see results this extreme or more by chance.  
  
In a frequentist approach you're asking “Is this effect statistically significant?”  
In an Information Theoretic approach you're asking “Which model best approximates reality?”  
  
  
**Here's how you treat models in the two approaches:**  
  
|                         | Frequentist Approach                                   | Information-Theoretic Approach                           |
|:-------------------------|:--------------------------------------------------------|:-----------------------------------------------------------|
| View of Models          | One model is “true”; others are incorrect              | All models are approximations of reality                  |
| Model Testing           | Typically uses null hypothesis testing (e.g., p-values)| Compares multiple plausible models simultaneously         |
| Model Fit Evaluation    | p-values, test statistics (e.g., t, F, χ²)             | AIC, AICc, BIC, log-likelihood                            |
| Selection Strategy      | Stepwise selection (e.g., backward, forward)           | Model ranking based on AIC, considers full model set      |
| Output Emphasis         | Significance tests, confidence intervals               | Akaike weights, evidence ratios, model averaging          |
  
  
**And here's how you treat data in the two approaches:** 
  
|                       | Frequentist Approach                                     | Information-Theoretic Approach                          |
|:------------------------|:----------------------------------------------------------|:----------------------------------------------------------|
| Role of Data           | Used to test hypotheses and estimate parameters         | Used to estimate how well models explain observed data   |
| Replication View       | Assumes repeated sampling from a population             | Data are fixed; models are evaluated against them        |
| Probability Perspective| Probability as long-run frequency                       | Probability as a measure of model fit via likelihood     |
| Significance Thinking  | Emphasizes binary decisions (e.g., p < 0.05)            | Avoids dichotomous thinking; emphasizes model support    |
| Source of Uncertainty  | From data sampling                                       | From model selection and uncertainty among models        |
  
  
**Why information theoretic?**  
We use existing knowledge to inform _a priori_ the choice of predictor variables in our model (i.e., _before we look at the data_!), rather than using the data to inform our choice of variables. 

**Why model averaging?**  
The benefit of using multiple models is that they can be ranked and scaled to show model uncertainty over the set. *All* models are then used for inference. "If you have a large number of closely related models, such as in linear-regression based variable selection .... designation of a single best model is unsatisfactory because that 'best' model is often highly variable. That is, the model estimated to be best would vary from data set to data set, where replicate data sets would be collected under the same underlying process. In this situation, model averaging provides a relatively much more stabilized inference." (p.151, Burnham & Anderson 2002).

**IT-based MMI is particularly good for:** 

* observational data 
*	data with lots of potential predictor variables  
*	messy data  
*	small datasets  

... in other words, many ecological data sets.

Although IT-based MMI is not completely free of the disadvantages of stepwise methods, it suffers less from the following drawbacks of stepwise methods:  

* Stepwise methods ignore model uncertainty (they test only a fraction of all possible models). This problem is exacerbated in smaller datasets and in inter-related predictors.  
*	Stepwise methods use subjective critical values (e.g. p < 0.05), leading to the assumption of zero effects of terms not in the final model. This is unlikely to be true for ecological data.  
*	Stepwise methods can lead to parameter estimation bias, with the risk that the final model will contain overestimated effect sizes.  
&nbsp;


The IT approach involves _a priori_ model specification, then model selection, and finally estimation of parameters and their precision.  
&nbsp; 

**A note about p-values**  
Information theoretic approaches do not use p-values. They work on a 'weight of evidence' approach. Here's a quick explainer about why NOT to look at the p-values, even though they are given in the model output:  
Traditional significance testing (i.e. in frequentist statistics) assumes a fixed model, or in other words it assumes there is one 'true' model. In model averaging we blend models, so the sampling distribution of each parameter is not based on a single model’s assumptions - this means that traditional significance testing isn't reliable. Also, standard errors are used in the calculation of p-values but the SEs in model averaged output are not straightforward - they are unconditional SEs (these try to account for model selection uncertainty) which again makes significance testing unreliable. Another point is that if a parameter is not included in many models then its averaged coefficient will be shrunk towards zero - but this doesn't necessarily reflect its 'true' effect size or importance, so you can't rely on the p-value associated with it. So basically, ignore the p-values!!!
  
  
So, we'll assume that we've identified possible predictor variables through a literature search. **N.B.** This should be done _before_ you even do fieldwork! Once you've collected data you can do some basic data exploration (but NO data dredging or modelling!).  

We'll now load packages and the dataset and get modelling.


# Load packages

```{r Load-packages, message=FALSE}
rm(list=ls())
library(dplyr) # for easy data manipulation
library(tibble) # for easy data manipulation
#library(stringr) # for dealing with strings
library(MASS) # for stats
library(lme4) # for mixed effects models
library(arm) # for standardizing variables
library(MuMIn) # for multi-model inference
library(knitr)
library(kableExtra) # for pretty tables
library(here) # for a more robust & reproducible approach than relative paths
library(performance) # for checking overdispersion
```


# Generate some data

We're going to work with real site data from English agroforestry and monoculture fields, but we will generate fake abundance data to make modelling easier.  
Metadata for the site variables is on Figshare [here](https://figshare.com/articles/dataset/Transect_walk_data_collected_in_English_agroforestry_and_monoculture_fields_/28770587). In brief, there are five English farms, each of which has an agroforestry (AF) and monoculture control (MC) treatment, sampled over two years. 

Load and tidy the data and generate fake abundances.
```{r}
# Set seed for reproducibility
set.seed(123)

abundance_data <- read.csv(here::here("data", "solitary_bees_clean.csv"), header = TRUE) %>% 
  dplyr::mutate(
    date = trimws(date),  # remove any leading/trailing whitespace
    date = as.Date(date, format = "%Y-%m-%d"), # ensure date is formatted correctly
    site = factor(site), # set as factor
    treatment = factor(treatment), # set as factor
    year = factor(year), # set as factor
    landuse = factor(landuse),
    field = factor(field),
    # create a julian date (number of days since 1st March)
    jdate = difftime(date, as.Date(paste(01, 03, year, sep="-"),"%d-%m-%Y")),
    jdate = as.integer(jdate)
  ) %>% 
  # just retain the site data
  dplyr::select(site:snh, jdate) %>% 
  dplyr::relocate(jdate, .after = date) %>% 
  # remove year 2013
  dplyr::filter(!year == "2013") %>% 
  droplevels() %>% 
  # fill in the missing data on boundary area from site WAF - use made-up values
  dplyr::mutate(bound_area = ifelse(
    is.na(bound_area) & site == "WAF" & year == 2011 & treatment == "AF", 2000, #boundary area was 2000m2
    ifelse(is.na(bound_area) & site == "WAF" & year == 2011 & treatment == "MC", 2050, bound_area) #boundary area was 2050m2
  )) %>% 
  # Generate some other variables that might affect sampled insect abundance
  dplyr::mutate(sun = ifelse(treatment == "AF",
                             rnorm(n(), mean = 60, sd = 10),
                             rnorm(n(), mean = 65, sd = 10)),
                sun = pmin(pmax(sun, 30), 100), # clamp between 30 and 100
                sun = as.integer(sun)) %>% 
  # Generate new abundance columns
  # create a fake site effect, too
  dplyr::mutate(
    butterfly_abundance = case_when(
      treatment == "AF" & site == "CE" ~ rpois(n(), lambda = 14),
      treatment == "AF" & site == "LHF" ~ rpois(n(), lambda = 10),
      treatment == "AF" & site == "RR" ~ rpois(n(), lambda = 13),
      treatment == "AF" & site == "SD" ~ rpois(n(), lambda = 18),
      treatment == "AF" & site == "WAF" ~ rpois(n(), lambda = 25),
      treatment == "AF" & site == "WH" ~ rpois(n(), lambda = 21),
      
      treatment == "MC" & site == "CE" ~ rpois(n(), lambda = 9),
      treatment == "MC" & site == "LHF" ~ rpois(n(), lambda = 7),
      treatment == "MC" & site == "RR" ~ rpois(n(), lambda = 15),
      treatment == "MC" & site == "SD" ~ rpois(n(), lambda = 11),
      treatment == "MC" & site == "WAF" ~ rpois(n(), lambda = 19),
      treatment == "MC" & site == "WH" ~ rpois(n(), lambda = 16),      
      TRUE ~ rpois(n(), lambda = 8)  # default fallback
    )
  )

# add in system age
# site <- c('WH','SD','WAF', 'RR', 'CE', 'LHF')
# age <- c(2,9,14,24,24,18)
# df <- data.frame (site, age)
# abundance_data <- merge(abundance_data, df, by="site")
```



# Inspect data
```{r}
glimpse(abundance_data)
```

What's the reference level for 'treatment'?
```{r}
levels(abundance_data$treatment)
```
It's agroforestry, because R automatically orders things alphabetically. Let's relevel the 'treatment' variable so that the monoculture is the reference level. This way, if agroforestry increases abundance, we will see a positive effect of AF in the model, rather than seeing a negative effect of monoculture.  
```{r}
abundance_data$treatment <- relevel(abundance_data$treatment, ref = "MC")
levels(abundance_data$treatment)
```



Treatments were sampled equally in each year:
```{r}
table(abundance_data$treatment, abundance_data$year)
```

Treatments within each site were sampled equally (two sampling events per site per month), but sites were not sampled equally:
```{r}
table(abundance_data$site, abundance_data$month, abundance_data$year)
```

Let's look at how many fields were in each site.
```{r}
abundance_data %>%
  group_by(site) %>%
  summarise(fields = paste(unique(field), collapse = ", ")) %>%
  arrange(site)
```
You can see that at some sites, both treatments were in the same field (e.g., at CE & RR, where there is only one field at each site). At other sites, the treatments were in different fields in different years (e.g., SD, WH).   
  
This is a typical ecological dataset: there are lots of other variables apart from treatment that might be affecting butterfly abundance, and some sites were sampled more often than others (although thankfully each treatment is sampled equally within sites). We're not going to focus on the sampling details here, but when using your own data you would of course need to think about how to account for  hierarchical designs, unequal sampling, lots of zeros in your data, etc. when considering your approach and model specification. The aim for this session is just to show you the mechanics of multi-model inference, which is particularly useful for datasets where there are lots of potentially influential variables.  
&nbsp; 

# Model

## 1. Build a highly parameterised global model

…but don’t overfit.

Use the predictor variables identified from (a) the literature, (b) your existing knowledge, and (c) any initial data exploration to build a 'global' model. Ideally, the global model has in it all the variables thought to be important, although it may be impossible to fit a global model if sample size is very small. In this example, the global model is a GLMM with a Poisson error structure because the data are counts of butterflies and have a hierarchical structure (fields within sites). Check goodness-of-fit, overdispersion, correlated variables etc. in the usual way. Because the data set used here is small, we can not include every potential predictor variable, so we will include only those which were likely to have the largest effect on butterfly abundance (based on the previous _a priori_ work).  

```{r Build-global-model-norescaling}
gm_notrescaled <- glmer(butterfly_abundance ~ treatment + landuse + bound_area + n_hedg_bound + sun + jdate + 
                                     treatment:landuse + (1|site), poisson, data=abundance_data,
                        na.action = "na.fail", 
                        glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1000000))
                       )
```

We get several warnings, one of which warns about the predictor variables being on different scales. We need to rescale them.  
  
## 2. Rescale variables
You can do this in several ways (the method you choose _may_ give you different parameter estimates):

* Use the [*scale*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) function in [base](https://www.rdocumentation.org/packages/base/versions/3.6.2) R. *Scale* gives values centered around zero by subtracting the mean of the variable from each value of the variable, then dividing by 1 standard deviation.
* Use the [*rescale*](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) function in the *scales* package, which rescales numeric vectors to have a specified maximum and minimum (default is 0-1).
* Use the [*standardize*](https://www.rdocumentation.org/packages/arm/versions/1.11-2/topics/standardize) function in the [*arm*](https://www.rdocumentation.org/packages/arm/versions/1.11-2/topics/standardize) package, which you can apply to a model rather than individual variables. It centers predictor variables and divides by 2 standard deviations. Numeric variables that take on more than two values are each rescaled to have a mean of 0 and a sd of 0.5; Binary variables are rescaled to have a mean of 0 and a difference of 1 between their two categories; Non-numeric variables that take on more than two values are unchanged; Variables that take on only one value are unchanged.  

We'll use *standardize* to rescale all variables:
```{r Rescale-global-model}
gm <- standardize(gm_notrescaled, standardize.y = FALSE) # we don't want to standardize the response variable.
summary(gm)
```


## 3. Check model fit

Check the model fit in the usual way. Model fit is assessed for the global model ONLY. If the global model fits the data adequately, then a more parsimonious model will also fit the data.
```{r Check-model}
par(mfrow=c(1,1))

qqnorm(resid(gm))
qqline(resid(gm))

hist(residuals(gm))
```

This will do for now - we're not concerned with model fit here as it's fake data anyway, we just want to learn the steps. However, we will check for overdispersion as we've used a Poisson error distribution (it won't be a problem here as we generated the data, but with your own data you should always check!). There are different ways to check overdispersion and you should do some reading on it if it applied to your data, but we'll use the 'check_overdispersion' function from the 'performance' package here.  
  
```{r}
performance::check_overdispersion(gm)
```
Once you are happy that your global model fits and doesn't violate any assumptions, you can go ahead with creating models for alternative hypotheses.  
  
## 4. Generate all possible lower-dimensional sub-models

...in effect, all possible hypotheses.  

We use the *dredge* function from the [*MuMIN*](https://www.rdocumentation.org/packages/MuMIn/versions/1.43.17) package, which is a multi-model inference package. You won't keep all of the sub-models generated, just those with the highest relative worth.  

This process can take a while (more predictors = more time).  
  
Because we have a small data set, we will use AICc to rank models.  
  
```{r Generate-model-set}
modset <- dredge(gm, rank = "AICc") # AICc is the default so 'rank =' is obsolete here. Rank options = AIC, QAIC, BIC.
head(modset, n=10)
```

The output shows the top 10 models of all the possible sub-models, ranked by AIC~*c*~.  
Column 1 is the model number in the set of generated sub-models. Column 2 is the estimated intercept. The following columns give the variables that are present in these models: where a variable is present in a model, the parameter estimate is given. Next are the degrees of freedom, the log likelihood ('logLik'), AIC~*c*~, the change in AIC~*c*~ ('delta'), and the Akaike weight of the model ('weight').  
The Akaike weight ($w$~*$i$*~) indicates the relative likelihood of a model, *given the data and the set of models*. Akaike weights are computed by normalising the likelihood values, and provide a way of scaling and interpreting the changes in AIC~*c*~ values. Akaike weights of all models in the set add up to 1, so if you add or remove a model from the set you need to recompute $w$~*$i$*~ for all the models in the new set. Unless the top model has $w$~*$i$*~ $\ge$ 0.9, you should use model averaging.

In the output above, we can see that the 'best' model in the set is not convincingly the best model: the evidence ratio for model 3 versus model 68 is only 1.1 (i.e., model 3 $w$~0.180~ / model 68 $w$~0.161~ = 1.118). This weak support for the 'best' model suggests that we would expect to see a lot of variation in the selected best model from sample to sample if we could draw multiple independent samples; in other words, the data-based model selection uncertainty is likely to be high. This indicates that these data are inadequate to reach strong inference so we should definitely use model averaging to make more robust inference (multi-model inference, or MMI).   
&nbsp; 

(An aside: note that the interaction we specified, 'treatment:landuse', appears in the model selection table as 'landuse:treatment'. This is just because MuMIn re-labels interaction terms alphabetically. It changes nothing in terms of model fitting, predictions, or interpretation).  
  
## 5. Create a 'top models' set

By comparing these models against each other to find their relative worth, we can refine the model set created in step 4 above, retaining just those models for which there is good empirical support. In this way we create a smaller set of candidate models to use for inference.  

Models in the set are ranked by AIC (or AIC~*c*~ for small sample sizes, or QAIC for overdispersed data, etc.).  
There is substantial empirical support for a model $i$ with difference in AIC from the top model ($\Delta$~$i$~) of 0-2, considerably less (but still some) support for models with $\Delta$~$i$~ = 4-7 and none for models with $\Delta$~$i$~ > 10 [(Burnham & Anderson 2002)](http://sutlib2.sut.ac.th/sut_contents/H79182.pdf). 

You can choose which cut-off to use to create your set of 'best' or 'top' models. Here we want our 'top models' set to contain only those models that have substantial empirical support so we'll choose $\Delta$~$i$~ $\le$ 2. In our case, this should pull out the top 6 models.  
```{r Get-top-models}
topmods <- get.models(modset, subset = delta <2)
```
  
How many models has this resulted in?
```{r}
length(topmods)
```
  
This is OK - we have 6 sites, so here _n_=6. You don't want the number of models to be greater than _n_.  
&nbsp; 

## 6. Use the top models set for inference  

There are four stages to this, the first two of which are done for you by the function *model.avg*:  

### 6.1 Make parameter estimates averaged across the top model set 
The function *model.avg* computes weighted estimates of the predicted values, weighting the predictions by the Akaike weights. The model output will show two sets of parameter estimates, which have been estimated as follows:  

* **Full average** Average a particular parameter over all models in the top models set, assuming a zero value for the parameter in models where it is absent (this serves to 'shrink' the parameter estimate back towards zero, which is useful in ameliorating model selection bias). Also called 'unconditional' average.  
* **Conditional average** Average a particular parameter over only those models in which it appears (the 'natural' average).  

In other words, the 'full' coefficients set terms to 0 if they are not included in the model while averaging, whereas the 'conditional' coefficients ignores the predictors entirely. The 'full' coefficients are thus more conservative.   

The 'full' method is useful if you're more interested in *which* parameters are having an effect. The 'conditional' (or natural average) method is useful if you're more interested in effect sizes (I'm still looking for the ref for this). In this example, the conclusions will be the same from either method. We'll use the conditional coefficients as I'm interested in effect sizes.  

Make sure to specify `fit = TRUE` in `model.avg()` so that the underlying models are stored for prediction later on (if you're not going to be predicting from the averaged model, you don't need this bit).  
```{r Model-average}
modavg <- model.avg(topmods, fit = TRUE)
summary(modavg)
```
  
Because we re-leveled the treatment variable, the intercept is treatment = monoculture, landuse = arable.  
The interaction term gives the unique effect of being in both pasture and agroforestry beyond the additive effects of pasture and agroforestry alone.  
 
Note that the output gives you unconditional SE ('adjusted SE'), which tries to account for model selection uncertainty. These are the ones to report.  
 
### 6.2 Quantify model selection uncertainty and incorporate this into estimates of precision 
If several other independent data sets were collected, would the same top model set be selected? Very possibly not. This model selection uncertainty must be included in measures of precision of parameter estimates: it is automatically included for you in the 'adjusted SE' (aka unconditional SE) values given in the output (see above).  

### 6.3 Calculate the relative importance of predictor variables.  
To calculate the relative variable importance ($w$~*$i$$p$*~) you simply sum the Akaike weights ($w$~*$i$*~) of the models in which the parameter appears.

For example, *treatment* (parameter 2) appears in all models in the top model set. You therefore add the Akaike weights of all models (`r round(Weights(modavg)[1],2)` + `r round(Weights(modavg)[2],2)` + `r round(Weights(modavg)[3],2)` + `r round(Weights(modavg)[4],2)` + `r round(Weights(modavg)[5],2)` + `r round(Weights(modavg)[6],2)`) to give relative variable importance $w$~*$i$$p$*~ = 1 for *treatment*.  
  
The *interaction* term (parameter 4) only appears in the second and fourth models (look at the component models and the term codes in the summary above). These models have $w$~*$i$$p$*~ = `r round(Weights(modavg)[2],4)` and $w$~*$i$$p$*~ = `r round(Weights(modavg)[4],4)` respectively. The relative importance ($w$~*$i$$p$*~) of the *interaction* between treatment and land use is therefore just `r round(Weights(modavg)[2],4)` + `r round(Weights(modavg)[4],4)` = 0.35 (i.e. not important).
  
The *MuMIn* package does these calculations for you:
```{r Get-relative-variable-importance}
sw(modavg)
```
  
The answers here are more accurate  than when doing it by hand (because by hand we used rounded values from the model summary).  
  
### 6.4 Calculate confidence intervals:
```{r CIs}
confint(modavg,level = 0.95)
```
  
When reporting your findings, talk about 'strength of evidence'. Avoid using the word 'significant' as we haven't done hypothesis testing and haven't used $p$ values. These have no place in this method. **Do** give estimates of effect size and associated precision.

# Create a table of results
You can present results in other ways, too, but this table gives all the important information and looks neat!
```{r}
# 1. Extract top model parameters and remove columns with all NAs
top.mod.params <- modset[1:length(topmods), ] %>%
  as.data.frame() %>%
  # remove columns (parameters) containing NA (i.e., no estimates for them in any of the top models)
  #dplyr::select(where(~ !all(is.na(.)))) # needs most up-to-date version of dplyr
  dplyr::select(which(colSums(!is.na(.)) > 0)) 

# 2. Create a table showing which parameters are included in each top model
modvars <- top.mod.params[, 1:(1 + length(sw(modavg)))] %>%  # Select only parameter columns
  t() %>%  # Transpose so parameters are rows
  as.data.frame() %>%
  rownames_to_column("Parameter") %>%
  setNames(c("Parameter", as.character(1:length(topmods)))) %>%  # Rename columns to model ranks
  mutate(across(-Parameter, ~ ifelse(!is.na(.), "\u2022", "")))  # Replace non-NA values with bullet points

# 3. Extract model-averaged estimates and standard errors
results <- MuMIn::coefTable(modavg) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Parameter") %>%
  dplyr::select(-df) %>%  # Drop unused df column
  dplyr::mutate(across(c("Estimate", "Std. Error"), ~ round(., 3)))  # Round numeric values

# 4. Standardize parameter names for joining
modvars <- modvars %>%
  dplyr::mutate(
    Parameter = gsub("X.Intercept.", "(Intercept)", Parameter),
    Parameter = gsub("\\.c", ":c", Parameter)
    )

# 5. Join model structure (bullets) with model-averaged results
results <- dplyr::left_join(results, modvars, by = "Parameter") %>%
  dplyr::relocate(c("Estimate", "Std. Error"), .after = last_col())  # Move numeric columns to the end

# 6. Add 95% confidence intervals
ci <- confint(modavg, level = 0.95) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Parameter") %>%
  dplyr::rename(LCI = `2.5 %`, UCI = `97.5 %`) %>%
  dplyr::mutate(across(c(LCI, UCI), ~ formatC(., format = "f", digits = 3, flag = "0")),
         `95%CI` = paste0("(", LCI, ", ", UCI, ")")) %>%
  dplyr::select(Parameter, `95%CI`)

results <- dplyr::left_join(results, ci, by = "Parameter")  # Add CI column to results

# 7. Add relative variable importance (wip)
varimp <- sw(modavg) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Parameter") %>%
  dplyr::rename(wip = ".") %>%
  dplyr::mutate(wip = formatC(round(wip, 2), format = "f", digits = 2, flag = "0"))

results <- left_join(results, varimp, by = "Parameter")  # Add wip column

# 8. Extract model fit statistics (df, logLik, AICc, delta, weight)
modgof <- top.mod.params %>%
  dplyr::select(df:weight) %>%
  dplyr::mutate(across(everything(), ~ formatC(as.numeric(.), format = "f", digits = 2, flag = "0"))) %>%
  t() %>%
  as.data.frame() %>%
  setNames(as.character(1:length(topmods))) %>%
  tibble::rownames_to_column("Parameter") %>%
  dplyr::mutate("Estimate" = NA, "Std. Error" = NA, `95%CI` = NA, wip = NA)  # Add empty columns to match results

# 9. Append model fit statistics to results
results <- bind_rows(results, modgof)

# 10. Format numeric columns and blank out cells in model fit rows
results <- results %>%
  dplyr::mutate(across(c("Estimate", "Std. Error"), ~ formatC(as.numeric(.), format = "f", digits = 3, flag = "0")),
         wip = formatC(as.numeric(wip), format = "f", digits = 2, flag = "0"))

fit_rows <- which(results$Parameter %in% c("df", "logLik", "AICc", "delta", "weight"))
results[fit_rows, which(names(results) %in% c("Estimate", "Std. Error", "95%CI", "wip"))] <- ""

# 11. Final cleanup of parameter names and column labels
results <- results %>%
  dplyr::rename("β" = "Estimate", SE = "Std. Error") %>%
  dplyr::mutate(
    Parameter = gsub("c\\.", "", Parameter),
    Parameter = gsub("z\\.", "", Parameter),
    Parameter = gsub("bound_area", "boundary area", Parameter))

# 12. Clean up environment
rm(top.mod.params, ci, varimp, modgof)
```

```{r Make-pretty-results-table, echo=FALSE, message=FALSE, warning = FALSE, }
mod_n <- length(topmods)
par_n <- nrow(modvars)

kable(results, booktabs = TRUE, align = c('l', rep('c', mod_n), rep('r', 4))) %>%
  kable_classic(full_width = FALSE, position = "left", html_font = "Arial") %>%
  add_header_above(c(" " = 1, "Model rank" = mod_n, "Model averaged coefficients" = 4)) %>%
  column_spec(2:(mod_n + 1), width = "1.7cm") %>%
  column_spec((mod_n + 2):(mod_n + 3), width = "1.5cm") %>%
  column_spec(mod_n + 4, width = "3cm") %>%
  column_spec(mod_n + 5, width = "1.3cm") %>%
  row_spec(par_n, extra_css = "border-bottom: 1px dotted") %>%
  row_spec(0, extra_css = "border-bottom: 1px solid")
```
Key: $\beta$, averaged parameter estimate; $w$<sub>$ip$</sub>, relative variable importance; SE, adjusted standard error; df, degrees of freedom; delta, change in AICc from top model; weight, Akaike weight. The levels of 'treatment' are _monoculture_ and _agroforestry_; the levels of 'landuse' are _arable_ and _pasture._ The intercept represents monoculture treatments in arable land use. 

Now, how could we have done that without R?!!
  
**Your final task** is to write a sentence or two explaining what the results tell us. Is treatment important? Are any other variables important? How can you tell?  
 
&nbsp; 

# Predict
If you want to predict from model averaged results, you can use the `predict` function just like you would for any other linear model.  
  
We standardised our independent variables using the `rescale` function, so that they had a mean of 0 and a SD of 0.5 (see section 2, where we rescaled the variables). This is how `rescale` does it, using boundary area as an example:  
  
`x_std = (x - mean(x, na.rm = TRUE)) / (2 * sd(x, na.rm = TRUE))`  
  
or  
  
`(abundance_data$bound_area - mean(abundance_data$bound_area)) / sd(abundance_data$bound_area)`
  
So, to interpret predictions meaningfully, we need to know the mean and SD of the original variable that we want to predict on. To show you how this works, we'll predict abundance in the two treatments for different amounts of boundary area.  
Remember, you can only predict for the variables included in the subset of models used for model averaging. You can't predict for variables in your original model that weren't present in the 'top' models subset.  
```{r}
# Get mean and SD of the original (unstandardized) bound_area variable
bound_area_mean_raw <- mean(abundance_data$bound_area, na.rm = TRUE)
bound_area_sd_raw <- sd(abundance_data$bound_area, na.rm = TRUE)
 
# Define the bound_area values you want to predict for: min, mean, max (in original scale)
bound_area_vals_raw <- c(min(abundance_data$bound_area, na.rm = TRUE),
                  bound_area_mean_raw,
                  max(abundance_data$bound_area, na.rm = TRUE))
 
# Standardize these values using the same transformation as your model
z_bound_area_vals <- (bound_area_vals_raw - bound_area_mean_raw) / bound_area_sd_raw
 
# Create a new data frame for prediction
## Always make sure the new data frame has:
## * Standardized versions of predictors (if model is standardized)
## * All required variables present and named correctly (must match what's in your model)
## * Matching factor levels
new_data <- expand.grid(
  z.bound_area = z_bound_area_vals, # standardized bound_area values
  c.treatment = c(0, 1),    # levels(abundance_data$treatment) gives "AF" "MC", so 0 = agroforestry, 1 = monoculture
  c.landuse = c(0, 1),      # levels(abundance_data$landuse) gives "a" "p", so 0 = arable and 1 = pasture
  `c.landuse:c.treatment` = c(0, 1)
)

# Predict biodiversity using the fixed effects from your model (our model is the model-averaged model, called 'modavg')
new_data$predicted_abundance <- predict(modavg, newdata = new_data, re.form = NA, type = "response") # 'type = "response"' back-transforms abundance estimates
 
# Add original boundary area values for interpretability
new_data$bound_area_raw <- rep(bound_area_vals_raw, each = 2)
 
# View the prediction table
print(new_data)
```

To make this easier to interpret, replace the 0s and 1s with meaningful factor levels.
```{r}
new_data <- new_data %>%
  dplyr::mutate(
    c.treatment = case_when(
      c.treatment == 0 ~ "AF",
      c.treatment == 1 ~ "MC"
    ),
    c.landuse = case_when(
      c.landuse == 0 ~ "arable",
      c.landuse == 1 ~ "pasture"
    )
  ) %>% 
  rename(
    treatment = c.treatment,
    landuse = c.landuse,
    `landuse:treatment` = `c.landuse:c.treatment`
  ) %>% 
  # change column order
  dplyr::relocate(z.bound_area, .after = bound_area_raw)

print(new_data)
```

The interaction term represents the unique effect of being in both pasture and agroforestry beyond the additive effects of pasture and agroforestry alone. The coefficient for the interaction term tells you how much the effect of one treatment changes depending on the level of landuse.

Explanation of the interaction term in this data frame:
'c.landuse' and 'c.treatment' are both binary variables (0 or 1), so their interaction behaves like this:

| c.landuse | c.treatment | c.landuse:c.treatment |
|:---------|:-----------|:----------------------|
|     0     |      0      |           0            |
|     0     |      1      |           0            |
|     1     |      0      |           0            |
|     1     |      1      |           1            |


Or, in other words:

| landuse  | treatment | landuse:treatment |
|:--------|:---------|:-----------------|
| arable   | MC        | no                |
| arable   | AF        | no                |
| pasture  | MC        | no                |
| pasture  | AF        | yes               |

  
# Useful references  
[Burnham & Anderson 2002](https://www.springer.com/gp/book/9780387953649)  
[Symonds & Moussalli 2011](https://link.springer.com/article/10.1007/s00265-010-1037-6)  
[Harrison et al 2018](https://peerj.com/articles/4794/)  
[Burnham et al 2011](https://link.springer.com/article/10.1007/s00265-010-1029-6)  
[Dormann et al 2018](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1309)    
  
# Glossary
  
**Sampling distribution**  
The entire distribution of the estimate over repeated samples. In more detail: This is the full probability distribution of an estimator (like a parameter estimate) across many hypothetical repeated samples from the population. It tells you how the estimate varies from sample to sample. The sampling distribution of each parameter changes when you average models because you’re combining information from multiple models, not relying on one fixed model (as you would in frequentist stats).  
  
**Standard error (SE)**  
The standard deviation (spread) of the sampling distribution. In more detail: SE is a summary measure of the sampling distribution — specifically, it’s the standard deviation of the sampling distribution. It quantifies the average amount of variability or uncertainty around the estimate.  
  
**Standard deviation (SD)**  
Measures the spread or variability of values around their mean in a data set or distribution. In other words, it tells you how much the individual values typically differ from the average. In model averaging, we aren’t typically computing the SD of raw data values. Instead, we're often concerned with the standard deviation of parameter estimates across models - so in IT approaches, SD refers to the variability of an estimated parameter across models.  
  
